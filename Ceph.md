Ceph - ПО, которое создаёт распределённое, отказоустойчивое хранилище объектов.

RBD - блочное устройство с поддержкой тонкого роста и снапшотами. ???? Виртуальный винчестер like.
Возможность быстрой миграции виртуальной машины между железными узлами. - RBD. WHY??????
CephFS - распределённая POSIX-совместимая файловая система.
RADOS Gateway - S3- и Swift-совместимый RESTful интерфейс.

Демоны Ceph.
MON - демон монитора,
модуль управления,
точка подключения клиентов.
OSD - демон хранилища.
MDS - сервер метаданных для CephFS.
MGR - демон менеджер,
метрики и
мониторинга.

**Устройство хранилища**
Pool. Имеет фактор репликации.
Состоит из placement group.
Placement group. Группа в которой хранятся объекты данных.
Связующее звено между физическим уровнем хранения данных (дисками) и логической организацией (пулами).
Прикреплены к разным OSD, в зависимости от фактора репликации.
Placement group объединяет объекты данных в группы, что позволяет уменьшить количество объектов при операциях внутри кластера.
Placement group хранит данные на нескольких OSD, количество которых равно параметру size.
Objects. Хранятся в pg_num.

Для пула необходимо указывать следующие параметры:
min_size. Минимальное количество копий данных, при наличии которых возможен доступ на запись.
Минимальное количество живых реплик, необходимое для работы пула.
size = Sets the number of replicas for objects in the pool. Количество копий данных.
pg_num = Количество placement group.
pgp_num
crush_rule

Controller replication under scalable hashing.
Карта репликации управляемых масштабируемым хэшированием.
Позволяет клиентам Ceph'a самостоятельно рассчитывать на каком OSD хранить данные и общаться с этим OSD напрямую.
Такой подход позволяет избежать единой точки отказа и избежать проблем, связанных с физическими лимитами на производительность отдельных серверов.
Можно использовать для создания пулов, которые хранят данные только на быстрых серверах или только на медленных.

Install Ceph
git clone scenario
requirements.txt

Переменные, настраивающие кластер находятся по пути **/inventory/group_vars/all.yml**
Откуда ставить репозиторий:
**ceph_origin: repository**
**ceph_repository: community**
**ceph_stable_release: luminous**
**public_network: "172.21.0.0/24"** - сеть, с которой кластер Ceph'a принимает коннекты от клиентов.
**cluster_network: "172.21.0.0/24"** - сеть, в которой компоненты Ceph'a обмениваются между собой информацией.

Демон для синхронизации времени:
**ntp_service_enabled: true**
**ntp_daemon_type: ntpd**

Каким образом хранить свои данные:
**osd_objectstore: bluestore** - есть ещё filestore, но переходят на bluestore.
**osd_scenario: lvm** - collocate and nocollocate - устаревшие сценарии.
При сценарии lvm под хранение данных создаются отдельные lvm-тома. При этом варианте сценарий Ansible пытается автоматически определить какой вариант размещения данных вам нужен.
Для этого он по умолчанию ищет все незадействованные диски, определяет их тип и создаёт на них разделы для хранения данных.
Например, если у вас есть hhd и ssd, то на первых он создаст раздел для хранения данных, а на ssd - раздел для хранения журналов.
Так же можно указать переменную devices, где перечислить на каких конкретно дисках вы хотите размещать свои данные.
**devices:**<br>
  - /dev/sdb<br>
В случае если указан блок **devices** сценарий Ansible не будет искать свободных дисков, а разместит только там, где указано.

Настройки кластера Ceph'a по умолчанию:<br>
ceph_conf_overrides:<br>
  global:
    osd_pool_default_pg_num: 32 - сколько placement group будет создано по умолчанию при создании пула.
    osd_pool_default_pgp_num: 32 - служебная настройка, используемая для разметки placement group. В документации рекомендуется делать её равной pg_num.
    osd_journal_size: 1024 - (в Мега Байтах)
    osd_pool_default_size: 3
    osd_pool_default_min_size: 2 - Количество живых реплик, при которых пул ещё работает. Если их становится меньше, пул прекрашает приём данных на запись до ребалансинга кластера.

##Вычисление pg_num##

Самая главная сложность при вычислении placement group - это соблюдение баланса между количеством групп на одной OSD и их размером.
Чем больше pg на одном OSD, тем больше нужно памяти, чтобы хранить информации об их расположении.
Чем больше размер placement group, тем больше данных будут перемещаться при процедуре балансировки кластера.
Т. о. мало placement group - они большого размера.
Много placement group - нужно больше памяти для демонов OSD.
Теоретическая формула - 1 Гб ОЗУ на 1 Тб памяти.

https://ceph.com/pgcalc/
Пресуппозиция:
принято считать, что на одном OSD оптимально хранить от 100 до 300 placement group.
Т. о. общее количество placement group в кластере (на примере калькулятора ceph.com, берущего за основу 100 pg на OSD) вычисляется по формуле:
Total PGs = (Total_number_of_OSD * 100) / max_replication_count

Более актуальные данные, где можно поиграться и посмотреть на изменения данных в пуле:
https://old.ceph.com/pgcalc/
Т. о. необходимо:
выделять на каждый пул количество PG, пропорционально количеству данных в них
pg_num = Total PGs * % of Size_of_pool/Total_size // Свериться с новыми данными на всякий случай.

прогнозировать изменение количества данных в пулах в будущем

Операция изменения количества placement group присутствует в Ceph, но достаточно ресурсоёмкая.
Во время ребалансинга данных в кластере, половина кластера будет копировать данные из одной placement group в другую.
Поэтому авторы Ceph'a советуют использовать количество групп, равное степеням 2. Т. о. ceph половину данных оставит на старом месте, половину перевезёт.

Основной файл для запуска - site.yml

Команды:
ceph health
ceph -s - информация о кластере
ceph df - информация о количестве данных в кластере.

ceph osd pool create **kube** 32 - создание пула с указанием его имени и количества pg в нём.

ceph osd pool application enable **kube** kubernetes - разрешение на использование пула кубернетесом: enable <имя пула> <название приложения>

Теперь создадим пользователя, дадим ему права на доступ к пулу и сохраним ключ доступа в файл
ceph auth get-or-create client.user mon 'allow r, allow command "osd blacklist"' osd 'allow rwx pool=kube' | tee /etc/ceph/ceph.client.user.keyring
ceph auth get-or-create (получить или создать) client.<name> (имя пользователя с префиксом client)
и список разрешений: кому и на что в формате **mon 'allow rwx' osd 'allow rwx pool=name_pool' etc 'allow etc'**
При успешном выполнении команы появляется строка вида:
[client.our_user_name]
    key = asdfjkl;==
В целях удобства эту информацию нужно сохранить в файл **/etc/ceph/ceph.client.user.keyring** на сервер, к которому будем подключать RBD disk.

Создание диска в пуле:
rbd create **disk1** --size **1G** --pool **kube**

rbd list --pool **kube**

Перед монтированием необходимо поставить на мастер машину ПО Ceph'a:
репозиторий:
yum install -y centos-release-ceph-luminous
пакет:
yum install -y ceph-common

Команда подключения:
Убедиться, что есть файлы:
**/etc/ceph/ceph.conf** и **/etc/ceph/ceph.client.user.keyring**
rbd map **disk1** --pool=**kube** --id=**user** - указываем просто имя пользователя без префикса **.client**.
Ceph client сам автоматически поставит префикс, поищет в директории /etc/ceph/ файл **ceph.client.<user>.keyring** и извлечёт оттуда ключ доступа.

На выходе он сообщает имя нового блочного устройства, например:
**/dev/rbd0**

Теперь на этом блочном устройстве необходимо создать файловую систему:
mkfs.ext4 **/dev/rbd0**

Создаём папку, которую будем монтировать на мастер-машине:
mkdir **/mnt/rbd**
Создаём точку монтирования:
mount **/dev/rbd0 /mnt/rbd**

#Автомонтирование RBD:#

В файле **/etc/ceph/rbdmap** содержится информация о блочных устройствах, которые надо подключать при загрузке.
**echo "pool_name/disk_name id=user_name,keyring=/etc/ceph/ceph.client.user.keyring" >> /etc/ceph/rbdmap**

Включаем автозапуск скрипта rbdmap.
**systemctl enable rbdmap**
**systemctl start rbdmap**

**Монтирование файловой системы**
Добавляем монтирование файловой системы блочного устройства в /etc/fstab командой:
**echo "/dev/rbd/pool_name/disk_name /mnt/rbd ext4 noauto,noatime 0 0" >>/etc/fstab**

Всё, можно перезагрузить сервер и ввести команду **mount** или **df**, чтобы убедиться, что всё работает.

#Монтирование ceph fs#
Команды:
**ceph fs ls**
Должна вернуть название и названия пулов для хранения данных и метадаты.

**cephfs** - это одна общая файловая система.
Для разделения прав доступа различным пользователям назначаются права на отдельные директории это файловой системы.

**Создание пула для CephFS**
//Монтируем на node-1.

**ceph osd pool create "название пула для хранения данных, например, cephfs_data" 32**
**ceph osd pool create "название пула для хранения метаданных, например, cephfs_metadata" 32**
Дальше - создание новой CephFs с указанием пулов для хранения метаданных и самих данных:
**ceph fs new "имя файловой системы, например, cephfs" cephfs_metadata cephfs_data**
**ceph fs ls**

Создаем пользователя для доступа к cephfs:
**ceph auth get-or-create *client.fsuser* mon 'allow r' mds 'allow r, allow rw path=/data_path' osd 'allow rw pool=*cephfs_data*'**
Пользователь создан, ему выданы права чтение-запись на директорию /data_path, но сама директория не создаётся  автоматически.
Надо смонтировать корень cephfs с правами администратора и создать там директорию /data_path.

**mkdir -p /mnt/cephfs**
**mount.ceph "адрес монитора, можно указать все, например: 172.21.200.6":/ /mnt/cephfs -o name=admin,secret=`ceph auth get-key client.admin`**
**mount.ceph** - монтируем файловую систему ceph, / - монтируем корень cephfs. /mnt/cephfs - монтируем туда директорию cephfs на нашем сервере.
опции: под каким юзером монтируем: **-o name, secret**
**mkdir -p /mnt/cephfs/data_path**

Заходим на master-1, монтируем cephfs там с юзером fsuser.

?Получим ключ доступа для юзера **fsuser**:
?**ceph auth get-key client.fsuser**
?
?Сначала положим ключ доступа клиента **fsuser** в файл **/etc/ceph/fsuser.secret**
?**echo "<ключ доступа клиента fsuser>" >/etc/ceph/fsuser.secret**

**mkdir -p /mnt/cephfs**
**mount.ceph** \*\*\*.\*\*.\*\*\*.\*,\*\*\*.\*\*.\*\*\*.\*,\*\*\*.\*\*.\*\*\*.\***:/data_path /mnt/cephfs -o name=fsuser,secretfile=/etc/ceph/fsuser.secret**

#Автомонтирование CephFs#

Настройка монтирования директории cephfs при загрузке сервера master-1:

Сперва убедиться, что на соответствующей машине есть файл с ключом доступа для нужного *юзера*
**cat /etc/ceph/*fsuser*.secret**

Дальше - заполняем файл **/etc/fstab**:
**echo "**\*\*\*.\*\*.\*\*\*.\*,\*\*\*.\*\*.\*\*\*.\*,\*\*\*.\*\*.\*\*\*.\***:/ /mnt/cephfs ceph name=*fsuser*,secretfile=/etc/ceph/fsuser.secret,**\_**netdev,noatime 0 0">>/etc/fstab**
secterfile - опция, указывающая путь к файлу с секретом.
\_netdev - опция, которая говорит о том, что устройство - сетевое и монтировать его надо только после того, как смонтирован и запущен сетевой стек.

Если у вас останутся подмонтированы каталоги, не упомянутые в /etc/fstab, то во время выключения системы сначала будет потушен сетевой интерфейс и только потом начнутся попытки отмонитровать подключение сетевого диска. Эти попытки будут неудачными и будут повторятся  целых 30 минут, пока не истечет глобальный таймаут на server shutdown.
Если же точка монтирования есть в /etc/fstab, и у неё указана опция \_netdev, то такой каталог будет отмонтирован до выключения сетевого интерфейса.

#Мониторинг#

На каждом сервере, где установлен MGR, нужно установить zabbix:

yum install zabbix-sender

ceph zabbix config-set zabbix_host zabbix.slurm.io
ceph zabbix config-set identifier node-1.slurm.io
ceph mgr module enable zabbix

Documentation:
http://docs/ceph.com/docs/master/mgr/zabbix

ceph health             Здоровье
ceph -s          	    Статус всего
ceph df                 статистика по занятому месту
ceph auth list          список прав клиентов
ceph mon dump           список мониторов
ceph osd dump           список пулов, osd и триггеров
ceph osd tree           список OSD с весам алгоритма CRUSH
ceph tell osd.X bench 	тестирование скорости доступа к osd.X

https://docs.ceph.com/en/latest/rados/operations/monitoring/
