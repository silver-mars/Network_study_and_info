# Компоненты кластера:

* Etcd
key-value хранилище, в котором хранится вся информация о кластере.
* API-server
* Controller-manager
* Scheduler
* Kubelet
* Kube-proxy

+ Контейнеризация
Система контейнеризации, поддерживающая Container Runtime Interface.
* Сеть
Container Network Interface.
* DNS
Service Discovery (coredns - default).

# ETCD

Работает по протоколу RAFT.
Etcdctl - утилита управления кластером ETCD (в основном используется для проверки здоровья кластера и вохможности делать снапшоты).
Требует быстрых дисков и стабильного широкого линка.

# API-server

* Центральный компонент кластера kubernetes.
* Единственный, кто общается с Etcd.
* Работает по REST API
* Authentication and authorization
Так как он единственный, кто общается с Etcd, работает по TLS и занимается авторизацией и аутентификацией пользователей

**Встроенные подсказки**
Помимо прочего в API server встроена документация по всем объектам, которые мы можем создать в кластере.
Для получения этой документации используется команда **kubectl explain** <type>.<fieldName>[.<fieldName>]
Таким образом, например, мы можем посмотреть описание всех доступных полей в pod'е:
**kubectl explain pod**
Или в spec pod'а:
**kubectl explain pod.spec**
Можно также получить список всех API объектов доступных в кластере:
**kubectl api-resources**
И посмотреть описание при помощи explain любого неизвестного ресурса.
Так что, если у Вас есть kubectl и доступ к кластеру Kubernetes. Вы всегда можете не выходя за пределы консоли получить документацию по любому полю и объекту. Это очень сильно экономит время при работе с кластером.

# Controller-manager

Комплекс различных контроллеров, например:
* Node controller
* Replicaset controller
* Endpoints controller
(Когда создаётся объект (например, сервис) в фоне создаются эндпоинты
* etc
* GarbageCollector (например, удаляет 11-ые replicasets)

Почти для всех абстракций кубернетеса есть свой контроллер.
Общая роль контроллеров - следить за состоянием и созданием новых объектов в рамках своей зоны ответственности.
Controller-manager генерирует описания replicaset'ов и pod'ов из объекта deployment

# Scheduler

Назначает pods на ноды, учитывая:
* QoS
* Affinity / anti-affinity
* Requested resources
* Priority Class

**Priority Class**
Подам можно выставлять приоритет. Эвакуируются и запускаются в первую очередь поды с более высоким приоритетом.
Если ресурсов не хватает - в первую очередь гасятся поды с более низким приоритетом.

**QoS**
Если не задано реквестов и лимитов, поду присваивается класс Best Effort.
Если заданы реквесты и лимиты, но они не равны, присваивается Burstable.
Если равны - Quaranteed.

Для kubernetes наиболее важны поды с Quaranteed, затем приоритет у Burstable.

Scheduler делает watch в Kube-API. Как только видит, что появилась новая запись, начинает работу.
**Поле NodeName как правило заполняется после того, как scheduler выбрал в соответствии с правилами подходящую ноду.**
После чего Kube-API-server записывает это в Etcd.

Самый быстрый способ в случае проблем с controlplane кластера продиагностировать неисправность - это выполнить команду
**kubectl get componentstatuses**

# Kubelet

Работает на каждой ноде кластера.
Единственный компонент, работающий не в Docker (условно можно назвать systemd приложением).

**!!!**
также смотрит в Kube-API и когда видит запрос с созданием нового пода, где NodeName - это принадлежащий ему адрес, начинает действовать:
Отдаёт команды Docker daemon'у и фактически создаёт pods
(отдает Docker'у команды для запуска контейнеров).

Соответственно Kubelet может запускать pod'ы без участия API server
Постоянно мониторит происходящее с подами и транслирует статус в API-server.
Контролирует работу наших подов, следит за их health check'ами.

**Компоненты Kubernetes в порядке их подключения к процессу запуска приложения**

1. kubectl
2. API-server
3. Controller-manager
4. Scheduler
5. Kubelet

# Kube-proxy

Также как и остальные компоненты кластера смотрит в Kube-API server.
Как и Kubelet стоит на всех серверах.
Только один Controller-manager одновременно может быть мастером и выполнять работу в кластере

**Задачи:**
* Управляет сетевыми правилами на нодах
* Фактически реализует Service (ipvs и iptables)
**Kube-proxy реализует абстракцию service!**

Предварительный экскурс:
**Service** - это набор правил (ipbs/iptables)

Следовательно эти правила можно посмотреть (например, команда iptables).
**sudo iptables -t nat -S | grep <service's name>**
--table	-t table	table to manipulate (default: 'filter')
--list-rules -S [chain [rulenum]]

Пример для разбора механики работы сервиса на уровне iptables:

-A KUBE-SERVICES
  -d *.*.*.*/32 // трафик, приходяший на ip-адрес сервиса (тут запечатлён оный ip-адрес).
  -p tcp // по протоколу tcp
  -m comment --cometn "mynamespace/myservice:http cluster
  IP"
  -m tcp --dport 80 // на 80-ый порт
-j KUBE-SVC-UT6F43GJFBEDB03V // нужно отправлять на эту цепочку

Дальнейший пример. Если грепнуть по имени этой цепочки, то мы найдём ещё два правила:

-A KUBE-SVC-UT6A43GJFBEDB03V
  -m comment --comment "mynamespace/myservice:http"
  -m statistic
    --mode random --probability 0.5000000000 // приходящий трафик с вероятностью 50%
-j KUBE-SEP-MMYWB6DZJI4RZ5CQ // будет отправляться в эту цепочку

-A KUBE-SVC-UT6A43GJFBEDB03V
  -m comment --comment "mynamespace/myservice:http"
-j KUBE-SEP-J33LX377GA3DLDWM // всё остальное отправлять в эту цепочку

Таким образом, трафик приходящий по изначальному адресу сервиса *.*.*.*/32 и соответствующий требованиям отправляется в цепочку, состоящую из двух других правил.
Раскрываем дальше:

-A KUBE-SEP-MMYWB6DZJI4RZ5CQ
  -p tcp
  -m comment --comment "mynamespace/myservice:http"
  -m tcp
-j DNAT // приходящий адрес dnat'ится
  --to-destination 10.102.0.93:80 // на данный ip-адрес.

-A KUBE-SEP-J33LX377GA3DLDWM
  -p tcp
  -m comment --comment "mynamespace/myservice:http"
  -m tcp
-j DNAT
  --to-destination 10.102.3.49:80 // или на данный ip-адрес.

Сервис - это своего рода балансировщик трафика на поды, и в контексте iptables он представляет из себя набор правил, которые через mode random probability позволяет балансировать трафик на поды.
**kubectl -n mynamespace get po -o wide**
pod-1   1/1 Running 0   6h  10.102.0.93
pod-2   1/1 Running 0   6h  10.102.3.49

Подводя итог.
Сервис - это некий статический IP-адрес, который был выдан кубернетесом при его создании.
Помимо ip-адреса у сервиса есть DNS имя в kube-dns (example: myservice.mynamespace.svc.cluster.local, "cluster.local" - доменное имя кластера)
DNS имя нужно чтобы иметь возможность обратиться к группе подов, стоящих за этим именем, используя только имя сервиса.
Так, если сделать **nslookup myservice**, то можно попасть в какой-то под.
Сервис - это просто правила iptables для роутинга, которые создаются на серверах. Это - не прокси.

# Network

Коротко можно охарактеризовать **цель сети кубернетес** как возможность любого пода с любой ноды обращаться к любому другому поду на любой ноде.
Соответственно коротко **цель сетевого плагина** - раздавать ip-адреса и налаживать взаимодействие между подами и нодами.
Также возможна реализация шифрования между нодами и управлять сетвыми политиками **(Network Policies)** (kalico).

Типичные задачи - попасть из сетевого нэймспейса контейнера в сетевой нэймспейс хоста.
Для этой реализации обычно используется veth-пары, которая одним концом воткнута в интерфейс контейнера, а другим - в bridge ноды.

**Container Network Interface**
Кубернетес занимается оркестрацией контейнеров - то есть организует запуск контейнеров на каком либо сервере-узле кластера.
Проблема состоит в том, что эти сервера могут находится где угодно - в одной локальной сети, в нескольких разных локальных сетях или просто раскиданы по всему интернету...
Способов организации сетевого взаимодействия между узлами кластера и контейнерами, запущенными на этих узлах может быть очень много, и поэтому сам кубернетес сетью не занимается.

Вместо этого придумали стандарт CNI (Container Network Interface), в котором описывается порядок создания сети в контейнерах, принадлежащих одному поду, каким образом назначаются IP адреса и настраивается маршрутизация внутри сетевого пространства пода.

Все контейнеры пода работают в одной network cgroup. Можно сказать, что внутри всех контейнеров находится один и тот же сетевой интерфейс.
Именно поэтому nginx из одного контейнера может слать запросы к php-fpm, запущенному в другом контейнере, на localhost:9000

Программные продукты, реализующие стандарт CNI называют CNI plugin.

Причем кроме задачи назначения IP адресов подам, эти плагины также занимаются задачей построения межузловой сетевой связности в кластере, чтобы обеспечить сетевое взаимодействие между подами, находящимися на разных узлах кластера.


